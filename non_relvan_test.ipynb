{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "2351ae85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0), tensor(0), tensor(0), tensor(0)]"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "dims  = [3, 5, 7, 3]\n",
    "keep_blocks = torch.tensor([1, 1, 0, 0], dtype=torch.bool)\n",
    "\n",
    "B = 4\n",
    "logits = torch.randn(B, sum(dims))\n",
    "\n",
    "device = logits.device\n",
    "dims_t = torch.as_tensor(dims, dtype=torch.long, device=device)\n",
    "keep_blocks = keep_blocks.to(device)\n",
    "\n",
    "# --- build per-category mask ---\n",
    "# Start with: kept blocks -> all True, masked blocks -> all False\n",
    "per_cat_mask = torch.repeat_interleave(keep_blocks, dims_t)  # [sum(dims)]\n",
    "\n",
    "# For masked blocks, allow exactly one category (within-index = 0)\n",
    "offsets = torch.nn.functional.pad(dims_t.cumsum(0), (1, 0))[:-1]  # start index of each block\n",
    "K = int(dims_t.sum().item())\n",
    "\n",
    "allow_one = torch.zeros(K, dtype=torch.bool, device=device)\n",
    "allow_one[offsets[~keep_blocks]] = True  # keep the first category of each masked block\n",
    "\n",
    "# Final mask: all cats from kept blocks, plus one cat from each masked block\n",
    "per_cat_mask = per_cat_mask | allow_one  # [sum(dims)]\n",
    "\n",
    "# Broadcast to batch if needed\n",
    "while per_cat_mask.dim() < logits.dim():\n",
    "    per_cat_mask = per_cat_mask.unsqueeze(0)\n",
    "per_cat_mask = per_cat_mask.expand_as(logits)\n",
    "\n",
    "# Mask logits and build the single Categorical\n",
    "masked_logits = logits.masked_fill(~per_cat_mask, float('-inf'))\n",
    "dist = Categorical(logits=masked_logits.flatten())\n",
    "\n",
    "sample = dist.sample()\n",
    "\n",
    "def unflatten(dims, sample):\n",
    "    indexes = []\n",
    "    reminder = sample\n",
    "    for i, d in enumerate(dims):\n",
    "        idx = reminder % d\n",
    "        indexes.append((idx))\n",
    "        reminder //= d\n",
    "    return indexes\n",
    "\n",
    "unflatten(dims, sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031d9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 0, 4],\n",
      "        [0, 3, 0, 1],\n",
      "        [0, 3, 0, 0],\n",
      "        [0, 2, 0, 2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# build all combos\n",
    "dims = [3,4,5,6]\n",
    "mask = [0,1,0,1]\n",
    "keep_blocks = torch.tensor(mask, dtype=torch.bool)\n",
    "\n",
    "B = 4\n",
    "logits = torch.randn(B, sum(dims), device=device)\n",
    "\n",
    "grids = torch.cartesian_prod(*[torch.arange(d, device=device) for d in dims])  # [315, 4]\n",
    "# keep only combos with masked blocks == 0\n",
    "mask_rows = torch.ones(len(dims), dtype=torch.bool, device=device)\n",
    "mask_rows = ~keep_blocks\n",
    "valid = (grids[:, mask_rows] == 0).all(dim=-1)\n",
    "grids = grids[valid]  # [315', 4], here 315' = 3*5*1*1 = 15\n",
    "\n",
    "# compute joint logits for each combo by summing per-block logits\n",
    "offsets = torch.nn.functional.pad(torch.tensor(dims, device=device).cumsum(0), (1,0))[:-1]\n",
    "pieces = []\n",
    "for i, d in enumerate(dims):\n",
    "    start = offsets[i]\n",
    "    block_logits = logits[:, start:start+d]                      # [B, d]\n",
    "    idx = grids[:, i].unsqueeze(0).expand(logits.size(0), -1)    # [B, 315']\n",
    "    pieces.append(block_logits.gather(1, idx))                   # [B, 315']\n",
    "joint_logits = sum(pieces)                                       # [B, 315']\n",
    "\n",
    "joint_dist = Categorical(logits=joint_logits)                    # batched\n",
    "joint_sample = joint_dist.sample()                               # [B], each in [0..315'-1]\n",
    "# convert back to per-block indices:\n",
    "idxs = grids[joint_sample]                                       # [B, 4], columns 2,3 are 0\n",
    "\n",
    "print(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45c8faf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16, 19, 18, 14], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "be3ebff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 0, 0],\n",
       "        [2, 3, 0, 0],\n",
       "        [0, 4, 0, 0],\n",
       "        [2, 3, 0, 0]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "98d821dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 0, 0],\n",
       "        [2, 1, 0, 0],\n",
       "        [2, 4, 0, 0],\n",
       "        [2, 0, 0, 0]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a891374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0337,  0.0649, -0.1024,  0.9784,  0.2401, -0.2484,  0.3114,  0.7636,\n",
      "         -1.0020, -1.1478,  1.1529,  0.1975, -0.9572,  1.3498, -0.9502, -1.3087,\n",
      "         -0.0481,  0.4834],\n",
      "        [ 0.0043, -0.4993,  1.7473,  1.0211,  0.2234,  0.0693, -0.9913,  0.6352,\n",
      "         -0.0644,  0.2137, -2.0399, -1.4999, -0.4451, -0.7589, -1.1711, -1.6983,\n",
      "         -1.2267, -1.0132]])\n",
      "tensor([[2, 0, 2, 0],\n",
      "        [2, 0, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class MaskedFlatJointCat:\n",
    "    \"\"\"\n",
    "    One Categorical over the joint (sum(dims)) with whole-block masking.\n",
    "    - Keeps only categories from blocks where keep_blocks[i] is True.\n",
    "    - Sampling is from a single Categorical over the KEPT categories.\n",
    "    - Returned samples can be reshaped to per-block indices with masked blocks set to 0.\n",
    "    - Log-probs are those of the single Categorical (proper renormalization over kept cats).\n",
    "    \"\"\"\n",
    "    def __init__(self, logits, dims, keep_blocks):\n",
    "        \"\"\"\n",
    "        logits: [..., sum(dims)]\n",
    "        dims:   list/1D tensor of ints, e.g. [3,5,7,3]\n",
    "        keep_blocks: 1D bool/0-1 tensor of shape [num_blocks]\n",
    "        \"\"\"\n",
    "        self.logits = logits\n",
    "        self.device = logits.device\n",
    "        self.dtype  = logits.dtype\n",
    "\n",
    "        self.dims = torch.as_tensor(dims, device=self.device, dtype=torch.long)\n",
    "        self.num_blocks = self.dims.numel()\n",
    "\n",
    "        keep = torch.as_tensor(keep_blocks, device=self.device, dtype=torch.bool)\n",
    "        assert keep.shape == (self.num_blocks,), \"keep_blocks must be [num_blocks]\"\n",
    "        self.keep = keep\n",
    "\n",
    "        # Offsets so that full_flat = offsets[block] + within_block\n",
    "        self.offsets = torch.nn.functional.pad(self.dims.cumsum(0), (1,0))[:-1]  # [num_blocks], exclusive cumsum\n",
    "\n",
    "        # Per-category mask and kept indices in the FULL flat space\n",
    "        per_cat_mask = torch.repeat_interleave(self.keep, self.dims)             # [sum(dims)]\n",
    "        self.per_cat_mask = per_cat_mask\n",
    "        self.kept_full_idx = per_cat_mask.nonzero(as_tuple=False).squeeze(-1)    # [K_kept]\n",
    "        assert self.kept_full_idx.numel() > 0, \"At least one block must be kept.\"\n",
    "\n",
    "        # Inverse map: full -> kept (=-1 for dropped)\n",
    "        K_full = int(self.dims.sum().item())\n",
    "        self.full_to_kept = torch.full((K_full,), -1, device=self.device, dtype=torch.long)\n",
    "        self.full_to_kept[self.kept_full_idx] = torch.arange(self.kept_full_idx.numel(), device=self.device)\n",
    "\n",
    "        # Build the *single* categorical over kept categories\n",
    "        kept_logits = logits.index_select(-1, self.kept_full_idx)                # [..., K_kept]\n",
    "        self.dist = Categorical(logits=kept_logits)\n",
    "\n",
    "    # ---------- mappings ----------\n",
    "    def kept_to_full(self, k_kept):\n",
    "        return self.kept_full_idx[k_kept]\n",
    "\n",
    "    def full_to_block_pair(self, k_full):\n",
    "        # block = index of offsets such that offsets[b] <= k_full < offsets[b]+dims[b]\n",
    "        b = torch.bucketize(k_full, self.offsets[1:])        # [same shape as k_full]\n",
    "        i = k_full - self.offsets[b]\n",
    "        return b, i\n",
    "\n",
    "    def block_pair_to_full(self, b, i):\n",
    "        return self.offsets[b] + i\n",
    "\n",
    "    def block_pair_to_kept(self, b, i):\n",
    "        k_full = self.block_pair_to_full(b, i)\n",
    "        return self.full_to_kept[k_full]                     # -1 if that (b,i) was masked out\n",
    "\n",
    "    # ---------- sampling ----------\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          per_block: sample reshaped to [..., num_blocks] (masked blocks are 0)\n",
    "          info: dict with flat/kept indices to avoid ambiguity when within==0\n",
    "        \"\"\"\n",
    "        batch_shape = self.logits.shape[:-1]\n",
    "        k_kept = self.dist.sample(sample_shape)              # sample in KEPT space\n",
    "        k_full = self.kept_to_full(k_kept)                   # map to FULL flat index\n",
    "        b, i = self.full_to_block_pair(k_full)               # block id + within-block id\n",
    "\n",
    "        per_block = torch.zeros(sample_shape + batch_shape + (self.num_blocks,),\n",
    "                                dtype=torch.long, device=self.device)\n",
    "        per_block[..., b] = i                                # masked blocks remain 0\n",
    "        return per_block, {\"k_kept\": k_kept, \"k_full\": k_full, \"block\": b, \"within\": i}\n",
    "\n",
    "    # ---------- log-prob ----------\n",
    "    def log_prob_from_kept(self, k_kept):\n",
    "        return self.dist.log_prob(k_kept)\n",
    "\n",
    "    def log_prob_from_block_pair(self, b, i):\n",
    "        \"\"\"\n",
    "        Log-prob of the assignment that selects block b with within-block index i.\n",
    "        (This is the correct way to score a 'reshaped' sample.)\n",
    "        \"\"\"\n",
    "        # invalid if block is masked\n",
    "        bad_block = ~self.keep[b]\n",
    "        # map (b,i) -> kept index\n",
    "        k_kept = self.block_pair_to_kept(b, i)\n",
    "        bad_pair = (k_kept < 0)\n",
    "\n",
    "        lp = self.dist.log_prob(k_kept.clamp_min(0))         # temp value\n",
    "        lp = torch.where(bad_block | bad_pair, torch.tensor(float('-inf'), device=self.device, dtype=self.dtype), lp)\n",
    "        return lp\n",
    "\n",
    "    def log_prob_from_full(self, k_full):\n",
    "        \"\"\"\n",
    "        Log-prob directly from FULL flat index (0..sum(dims)-1).\n",
    "        \"\"\"\n",
    "        k_kept = self.full_to_kept[k_full]\n",
    "        bad = (k_kept < 0)\n",
    "        lp = self.dist.log_prob(k_kept.clamp_min(0))\n",
    "        lp = torch.where(bad, torch.tensor(float('-inf'), device=self.device, dtype=self.dtype), lp)\n",
    "        return lp\n",
    "\n",
    "\n",
    "dims = [3,5,7,3]\n",
    "keep = torch.tensor([1,1,1,0])\n",
    "B = 2\n",
    "logits = torch.randn(B, sum(dims))\n",
    "\n",
    "\n",
    "dist = MaskedFlatJointCat(logits, dims, keep)\n",
    "\n",
    "# --- Sample\n",
    "per_block, info = dist.sample()   # per_block: [B, 4], last two are 0 (masked)\n",
    "print(per_block)                  # e.g. tensor([[0, 3, 0, 0],\n",
    "                                  #               [2, 0, 0, 0]])\n",
    "# info[\"k_full\"] is the full flat index; info[\"block\"], info[\"within\"] disambiguate zeros.\n",
    "\n",
    "# --- Log-prob\n",
    "lp1 = dist.log_prob_from_kept(info[\"k_kept\"])            # direct\n",
    "lp2 = dist.log_prob_from_full(info[\"k_full\"])            # via full flat index\n",
    "lp3 = dist.log_prob_from_block_pair(info[\"block\"], info[\"within\"])  # via reshaped pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "211955da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 0, 3, 0],\n",
       "         [1, 0, 3, 0]]),\n",
       " {'k_kept': tensor([11,  1]),\n",
       "  'k_full': tensor([11,  1]),\n",
       "  'block': tensor([2, 0]),\n",
       "  'within': tensor([3, 1])})"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
